Datasets: SST-2 (GLUE) \citep{wang2018glue} with BERT-base-uncased \citep{devlin2019bert}, WikiText-2 \citep{merity2016wikitext} with GPT-2-small \citep{radford2019gpt2}, and a synthetic tabular dataset. Explainers: Integrated Gradients \citep{sundararajan2017integrated}, SHAP \citep{lundberg2017shap}, LIME \citep{ribeiro2016lime}, Random. We compute $\Fscore$ on 200 instances for SST-2 and WikiText-2 (coverage runs for SHAP/LIME), and perform ROAR-based validation \citep{hooker2019roar}. Configurations, seeds, and code are released for reproducibility. Statistical analysis includes paired t-tests with Bonferroni correction, Wilcoxon signed-rank tests, Cohen's d, and bootstrap 95\% CIs.

\paragraph{Configuration.} Unless otherwise stated, we use random seed 42 and CPU execution. For faithfulness estimation we draw 64 Monte Carlo samples per input for main runs (32/16 for small coverage runs) with random baselines and PAD masking for text. SST-2 uses BERT-base-uncased fine-tuned on SST-2; WikiText-2 uses GPT-2-small. SHAP/LIME are run on a 25-example subset to provide coverage given computational constraints; Integrated Gradients and Random are run on 200 examples for both datasets.

\paragraph{Procedure.} For each input, we compute explainer attributions and evaluate $\Fscore$ by masking the top-fraction features per explainer ranking under random-baseline Monte Carlo sampling. We aggregate per-example scores into dataset-level summaries (mean, median, quantiles) with 95\% bootstrap CIs. We then conduct paired significance tests between methods and validate via ROAR correlation, where available, using probability/accuracy drops after feature removal.

\paragraph{Reproducibility.} We fix random seed 42 throughout and record all configurations in JSON artifacts alongside results. We report dataset sizes (n=200 for main SST-2 and WikiText-2 runs) and use CPU inference to ensure portability. Scripts to reproduce are provided; see Appendix for exact commands.

